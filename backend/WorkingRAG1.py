from langchain.memory import ConversationBufferMemory
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from operator import itemgetter

# Initialize the language model and memory for conversation
llm = ChatOpenAI(
    streaming=True,
    base_url="http://localhost:1234/v1",  # Ensure this matches the LM Studio port
    api_key="lm-studio",  # Placeholder for local LM Studio instance
    temperature=0,
    model="lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF"  # Use the model available in LM Studio
)

# Create a prompt template
template = """
You are a helpful AI assistant. You always fulfill the user's requests to the best of your ability.
Remember our previous conversation and use that context to provide informative and relevant answers.

Question: {question}

{history}
"""

chat_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", template),
        ("human", "{question}"),
    ]
)

# Initialize conversation memory
memory = ConversationBufferMemory(memory_key="history", return_messages=True)

def predict(question, history, request):
    print("Debug: Entered predict function")
    print(f"Debug: Received question: {question}")
    print(f"Debug: Received history: {history}")

    # Initialize conversation memory with history
    if history:
        for human, ai in history:
            print(f"Debug: Adding to memory - Human: {human}, AI: {ai}")
            memory.chat_memory.add_user_message(human)
            memory.chat_memory.add_ai_message(ai)

    try:
        # Create the chain for conversation
        print("Debug: Creating conversation chain...")
        chain = (
            {
                "context": RunnablePassthrough(),
                "question": RunnablePassthrough()
            }
            |
            RunnablePassthrough.assign(
                history=RunnableLambda(memory.load_memory_variables) | itemgetter("history")
            )
            | chat_prompt
            | llm
        )

        # Generate response
        partial_message = ""
        print("Debug: Starting to stream response from LM Studio...")
        for response in chain.stream(question):
            partial_message += response.content
            print(f"Debug: Partial response received: {partial_message}")
            yield partial_message

        # In case no response was generated
        if not partial_message:
            print("Debug: No response was generated by LM Studio.")
            yield "The model did not generate a response. Please try again."

    except Exception as e:
        print(f"Error in predict function: {e}")
        yield f"An error occurred while processing your request: {e}"
